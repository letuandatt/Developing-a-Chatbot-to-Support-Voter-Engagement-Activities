import streamlit as st
import sys
import os
import re
import json
import uuid
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.schema import Document


IGNORE_PATTERNS = [
    re.compile(r"^\s*Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM\s*$", re.IGNORECASE),
    re.compile(r"^\s*Äá»™c láº­p - Tá»± do - Háº¡nh phÃºc\s*$", re.IGNORECASE),
    re.compile(r"^\s*THá»¦ TÆ¯á»šNG CHÃNH PHá»¦\s*$", re.IGNORECASE),
    re.compile(r"^\s*KÃ½ bá»Ÿi: Cá»•ng ThÃ´ng tin Ä‘iá»‡n tá»­ ChÃ­nh phá»§\s*$", re.IGNORECASE),
    re.compile(r"^\s*Email: thongtinchinhphu@chinhphu\.vn\s*$", re.IGNORECASE),
    re.compile(r"^\s*CÆ¡ quan: VÄƒn phÃ²ng ChÃ­nh phá»§\s*$", re.IGNORECASE),
    re.compile(r"^\s*Thá»i gian kÃ½: \d{2}\.\d{2}\.\d{4} \d{2}:\d{2}:\d{2} \+\d{2}:\d{2}\s*$", re.IGNORECASE),
]

# Cáº¥u hÃ¬nh giao diá»‡n
st.set_page_config(
    page_title="Legal RAG Assistant",
    page_icon="âš–ï¸",
    layout="wide"
)


@st.cache_resource
def load_embedding_model():
    """Load vÃ  cache embedding model"""
    return HuggingFaceEmbeddings(
        model_name='bkai-foundation-models/vietnamese-bi-encoder',
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )


@st.cache_resource
def load_prebuilt_vectorstore(chunks_file_path: str, show_messages: bool = True):
    """Load vectorstore tá»« file chunks Ä‘Ã£ cÃ³ sáºµn"""
    try:
        if not os.path.exists(chunks_file_path):
            if show_messages:
                st.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file chunks: {chunks_file_path}")
            return None

        embeddings = load_embedding_model()
        if not embeddings:
            if show_messages:
                st.error("âŒ KhÃ´ng thá»ƒ load embedding model")
            return None

        # Äá»c chunks tá»« file JSONL
        chunks = []
        with open(chunks_file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        chunk = json.loads(line)
                        chunks.append(chunk)
                    except json.JSONDecodeError as e:
                        if show_messages:
                            st.warning(f"âš ï¸ Lá»—i parse JSON á»Ÿ dÃ²ng {line_num}: {str(e)}")
                        continue

        if not chunks:
            if show_messages:
                st.error("âŒ File chunks rá»—ng hoáº·c khÃ´ng Ä‘á»c Ä‘Æ°á»£c")
            return None

        if show_messages:
            st.info(f"ğŸ“Š ÄÃ£ Ä‘á»c {len(chunks)} chunks tá»« file")

        # Táº¡o Documents cho Chroma
        documents = []
        for i, chunk in enumerate(chunks):
            try:
                doc = Document(
                    page_content=chunk.get('content', ''),
                    metadata={
                        'source': chunk.get('source', 'N/A'),
                        'location': chunk.get('location', 'N/A'),
                        'id': chunk.get('id', str(uuid.uuid4()))
                    }
                )
                documents.append(doc)
            except Exception as e:
                if show_messages:
                    st.warning(f"âš ï¸ Lá»—i xá»­ lÃ½ chunk {i}: {str(e)}")
                continue

        if not documents:
            if show_messages:
                st.error("âŒ KhÃ´ng táº¡o Ä‘Æ°á»£c documents tá»« chunks")
            return None

        # Táº¡o vector store
        if show_messages:
            with st.spinner(f"ğŸ”„ Äang táº¡o vectorstore tá»« {len(documents)} documents..."):
                vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding=embeddings,
                    persist_directory=None  # In-memory store
                )
        else:
            vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=embeddings,
                persist_directory=None
            )

        if show_messages:
            st.success(f"âœ… ÄÃ£ load {len(chunks)} chunks vÃ o vectorstore")
        return vectorstore

    except Exception as e:
        if show_messages:
            st.error(f"âŒ Lá»—i load vectorstore: {str(e)}")
            import traceback
            st.error(f"Chi tiáº¿t lá»—i: {traceback.format_exc()}")
        return None


def generate_response_prebuilt(prompt: str, vectorstore) -> str:
    """Táº¡o pháº£n há»“i tá»« pre-built RAG system"""
    try:
        if not vectorstore:
            return "âŒ Vectorstore chÆ°a Ä‘Æ°á»£c load. Vui lÃ²ng kiá»ƒm tra láº¡i file chunks."

        # TÃ¬m kiáº¿m relevant documents
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # Láº¥y top 5 chunks liÃªn quan nháº¥t
        )

        relevant_docs = retriever.get_relevant_documents(prompt)

        if not relevant_docs:
            return "ğŸ¤” KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a báº¡n."

        # Táº¡o response tá»« relevant documents
        response_parts = [
            f"ğŸ“‹ **Tráº£ lá»i cho cÃ¢u há»i:** {prompt}\n",
            "ğŸ” **ThÃ´ng tin liÃªn quan tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t:**\n"
        ]

        for i, doc in enumerate(relevant_docs):
            source = doc.metadata.get('source', 'N/A')
            location = doc.metadata.get('location', 'N/A')
            content = doc.page_content

            # Giá»›i háº¡n Ä‘á»™ dÃ i ná»™i dung hiá»ƒn thá»‹
            if len(content) > 1000:
                content = content[:1000] + "..."

            response_parts.append(f"""
**{i + 1}. Tá»« {source} - {location}:**
{content}
""")

        response_parts.append(
            f"\nğŸ’¡ **Ghi chÃº:** ThÃ´ng tin Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« {len(relevant_docs)} Ä‘oáº¡n vÄƒn báº£n liÃªn quan nháº¥t trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.")

        return "\n".join(response_parts)

    except Exception as e:
        return f"âŒ Lá»—i xá»­ lÃ½ cÃ¢u há»i: {str(e)}"


def generate_response_upload(prompt: str) -> str:
    """Placeholder cho cháº¿ Ä‘á»™ upload file"""
    return ("ğŸ“„ **Cháº¿ Ä‘á»™ Upload File**\n\nTÃ­nh nÄƒng nÃ y Ä‘ang Ä‘Æ°á»£c phÃ¡t triá»ƒn. Hiá»‡n táº¡i báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cháº¿ Ä‘á»™ "
            "'Pre-built Database' Ä‘á»ƒ test cÃ¡c cÃ¢u há»i vá»›i dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn.")


# Khá»Ÿi táº¡o session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "mode" not in st.session_state:
    st.session_state.mode = "prebuilt"
if "vectorstore" not in st.session_state:
    # Auto-load vectorstore khi khá»Ÿi Ä‘á»™ng
    default_chunks_path = "ChiThi/final_chunks_for_embedding_semantic.jsonl"
    with st.spinner("ğŸ”„ Äang khá»Ÿi táº¡o há»‡ thá»‘ng..."):
        st.session_state.vectorstore = load_prebuilt_vectorstore(default_chunks_path, show_messages=False)

# Sidebar cho cáº¥u hÃ¬nh
with st.sidebar:
    st.title("âš™ï¸ Configuration")

    # Mode selection
    mode = st.radio(
        "Chá»n cháº¿ Ä‘á»™ hoáº¡t Ä‘á»™ng:",
        ["prebuilt", "upload"],
        format_func=lambda x: "ğŸ—‚ï¸ Pre-built Database" if x == "prebuilt" else "ğŸ“ Upload Files",
        index=0 if st.session_state.mode == "prebuilt" else 1
    )

    # Cáº­p nháº­t mode náº¿u thay Ä‘á»•i
    if mode != st.session_state.mode:
        st.session_state.mode = mode
        st.session_state.messages = []  # Clear chat history khi Ä‘á»•i mode
        st.rerun()

    st.markdown("---")

    if mode == "prebuilt":
        st.markdown("### ğŸ—‚ï¸ Pre-built Database")
        st.info("Sá»­ dá»¥ng cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ sáºµn")

        # Input path cho chunks file
        chunks_file_path = st.text_input(
            "ÄÆ°á»ng dáº«n file chunks (JSONL):",
            value="db/chroma_db",
            help="Nháº­p Ä‘Æ°á»ng dáº«n tá»›i file JSONL chá»©a cÃ¡c chunks Ä‘Ã£ xá»­ lÃ½"
        )

        # Load vectorstore button
        if st.button("ğŸ”„ Reload Vectorstore", type="primary"):
            with st.spinner("ğŸ”„ Äang reload vectorstore..."):
                vectorstore = load_prebuilt_vectorstore(chunks_file_path, show_messages=True)
                st.session_state.vectorstore = vectorstore
                if vectorstore:
                    st.success("âœ… Vectorstore Ä‘Ã£ Ä‘Æ°á»£c reload thÃ nh cÃ´ng!")

        # ThÃ´ng tin vá» vectorstore hiá»‡n táº¡i
        if st.session_state.vectorstore:
            st.success("âœ… Vectorstore Ä‘Ã£ sáºµn sÃ ng")
        else:
            st.warning("âš ï¸ Vectorstore khÃ´ng kháº£ dá»¥ng")
            st.info("CÃ³ thá»ƒ file chunks khÃ´ng tá»“n táº¡i hoáº·c cÃ³ lá»—i khi load")

    else:  # upload mode
        st.markdown("### ğŸ“ Upload Files")
        st.info("Upload file PDF Ä‘á»ƒ phÃ¢n tÃ­ch (Äang phÃ¡t triá»ƒn)")

        uploaded_file = st.file_uploader(
            "Upload PDF Document",
            type="pdf",
            help="TÃ­nh nÄƒng nÃ y Ä‘ang Ä‘Æ°á»£c phÃ¡t triá»ƒn",
            disabled=True
        )
        st.markdown("*TÃ­nh nÄƒng nÃ y sáº½ Ä‘Æ°á»£c hoÃ n thiá»‡n trong phiÃªn báº£n tiáº¿p theo*")

# Main interface
st.title("ğŸ” Legal RAG Assistant")
st.markdown("*Há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh vá» vÄƒn báº£n phÃ¡p luáº­t*")

# Hiá»ƒn thá»‹ tráº¡ng thÃ¡i dá»±a vÃ o mode
if st.session_state.mode == "prebuilt":
    if st.session_state.vectorstore:
        st.success("âœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng! Báº¡n cÃ³ thá»ƒ Ä‘áº·t cÃ¢u há»i vá» cÃ¡c vÄƒn báº£n phÃ¡p luáº­t.")
    else:
        st.error("âŒ KhÃ´ng thá»ƒ khá»Ÿi táº¡o vectorstore. Vui lÃ²ng kiá»ƒm tra file chunks hoáº·c reload á»Ÿ sidebar.")
else:
    st.info("ğŸ“ Cháº¿ Ä‘á»™ Upload Files (Äang phÃ¡t triá»ƒn)")

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("Há»i vá» vÄƒn báº£n phÃ¡p luáº­t..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate response based on mode
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” Äang tÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch..."):
            if st.session_state.mode == "prebuilt":
                response = generate_response_prebuilt(prompt, st.session_state.vectorstore)
            else:
                response = generate_response_upload(prompt)

        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

# Footer vá»›i hÆ°á»›ng dáº«n
st.markdown("---")
if st.session_state.mode == "prebuilt":
    st.markdown(
        "ğŸ’¡ **HÆ°á»›ng dáº«n:** Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng load dá»¯ liá»‡u. Báº¡n cÃ³ thá»ƒ Ä‘áº·t cÃ¢u há»i ngay hoáº·c reload vá»›i file khÃ¡c á»Ÿ sidebar.")
else:
    st.markdown("ğŸ’¡ **HÆ°á»›ng dáº«n:** Cháº¿ Ä‘á»™ Upload Ä‘ang Ä‘Æ°á»£c phÃ¡t triá»ƒn. Vui lÃ²ng sá»­ dá»¥ng cháº¿ Ä‘á»™ 'Pre-built Database'.")

# VÃ­ dá»¥ cÃ¢u há»i máº«u
with st.expander("ğŸ’¬ VÃ­ dá»¥ cÃ¢u há»i"):
    st.markdown("""
    **VÃ­ dá»¥ cÃ¡c cÃ¢u há»i báº¡n cÃ³ thá»ƒ thá»­:**
    - Chá»‰ thá»‹ 1722 nÃ³i vá» váº¥n Ä‘á» gÃ¬?
    - CÃ¡c quy Ä‘á»‹nh vá» kiá»ƒm soÃ¡t thá»§ tá»¥c hÃ nh chÃ­nh lÃ  gÃ¬?
    - Má»©c chuáº©n nghÃ¨o Ã¡p dá»¥ng cho giai Ä‘oáº¡n 2011-2015 nhÆ° tháº¿ nÃ o?
    - Ai lÃ  ngÆ°á»i kÃ½ Chá»‰ thá»‹ 1752?
    - Thá»i gian Ä‘iá»u tra há»™ nghÃ¨o Ä‘Æ°á»£c thá»±c hiá»‡n khi nÃ o?
    - Quy Ä‘á»‹nh vá» xá»­ lÃ½ vi pháº¡m trong lÄ©nh vá»±c xÃ¢y dá»±ng?
    - TrÃ¡ch nhiá»‡m cá»§a cÃ¡c bá»™, ngÃ nh trong thá»±c hiá»‡n chá»‰ thá»‹?
    """)

# Clear chat button
if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", key="clear_chat"):
    st.session_state.messages = []
    st.rerun()