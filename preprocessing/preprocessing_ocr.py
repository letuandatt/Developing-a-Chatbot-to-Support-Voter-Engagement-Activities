from io import BytesIO
import cv2
import numpy as np
from PIL import Image, ImageEnhance, ImageFilter
import fitz  # PyMuPDF
from vietocr.tool.predictor import Predictor
from vietocr.tool.config import Cfg
import os
import matplotlib.pyplot as plt
import re
import json
from datetime import datetime
from collections import defaultdict, Counter
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def extract_images_from_pdf_pages(pdf_path, start_page=0, end_page=None, dpi=400):
    """
    Chuy·ªÉn PDF th√†nh ·∫£nh v·ªõi DPI cao cho OCR
    """
    try:
        doc = fitz.open(pdf_path)
        images = []

        if end_page is None:
            end_page = len(doc)

        for page_num in range(start_page, min(end_page, len(doc))):
            page = doc.load_page(page_num)
            mat = fitz.Matrix(dpi / 72, dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("ppm")
            img = Image.open(BytesIO(img_data))

            images.append({
                'page_num': page_num + 1,
                'image': img,
                'width': img.width,
                'height': img.height
            })

        doc.close()
        return images
    except Exception as e:
        logger.error(f"L·ªói khi tr√≠ch xu·∫•t ·∫£nh t·ª´ PDF: {e}")
        return []

def advanced_preprocess_legal_document(image, debug_dir="debug_images"):
    """
    Ti·ªÅn x·ª≠ l√Ω m·∫°nh nh·∫•t cho t√†i li·ªáu ph√°p l√Ω, t·ªëi ∆∞u cho VietOCR
    """
    os.makedirs(debug_dir, exist_ok=True)

    if isinstance(image, Image.Image):
        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    else:
        img = image.copy()

    # 1. Chuy·ªÉn sang grayscale
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img.copy()
    cv2.imwrite(os.path.join(debug_dir, "1_grayscale.png"), gray)

    # 2. Kh·ª≠ nhi·ªÖu m·∫°nh
    denoised = cv2.bilateralFilter(gray, 11, 100, 100)
    cv2.imwrite(os.path.join(debug_dir, "2_denoised.png"), denoised)

    # 3. TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n v·ªõi CLAHE
    clahe = cv2.createCLAHE(clipLimit=4.5, tileGridSize=(8, 8))
    enhanced = clahe.apply(denoised)
    cv2.imwrite(os.path.join(debug_dir, "3_enhanced.png"), enhanced)

    # 4. L√†m m∆∞·ª£t nh·∫π
    smoothed = cv2.GaussianBlur(enhanced, (3, 3), 0)
    cv2.imwrite(os.path.join(debug_dir, "4_smoothed.png"), smoothed)

    # 5. Ng∆∞·ª°ng th√≠ch nghi
    thresh = cv2.adaptiveThreshold(
        smoothed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 7, 3
    )
    cv2.imwrite(os.path.join(debug_dir, "5_threshold.png"), thresh)

    # 6. L√†m s·∫°ch nhi·ªÖu
    kernel_noise = np.ones((2, 2), np.uint8)
    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel_noise)
    cv2.imwrite(os.path.join(debug_dir, "6_cleaned.png"), cleaned)

    # 7. L√†m ƒë·∫≠m vƒÉn b·∫£n
    kernel_dilate = np.ones((1, 1), np.uint8)
    cleaned = cv2.dilate(cleaned, kernel_dilate, iterations=2)
    cv2.imwrite(os.path.join(debug_dir, "7_dilated.png"), cleaned)

    # 8. L√†m m·ªèng l·∫°i
    kernel_erode = np.ones((1, 1), np.uint8)
    cleaned = cv2.erode(cleaned, kernel_erode, iterations=1)
    cv2.imwrite(os.path.join(debug_dir, "8_eroded.png"), cleaned)

    return cleaned

def preprocess_with_pil_enhancement(image, debug_dir="debug_images"):
    """
    Ti·ªÅn x·ª≠ l√Ω PIL t·ªëi ∆∞u cho VietOCR
    """
    os.makedirs(debug_dir, exist_ok=True)

    if isinstance(image, np.ndarray):
        if len(image.shape) == 3:
            pil_img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        else:
            pil_img = Image.fromarray(image)
    else:
        pil_img = image

    pil_img.save(os.path.join(debug_dir, "pil_original.png"))

    # TƒÉng ƒë·ªô s·∫Øc n√©t
    enhancer = ImageEnhance.Sharpness(pil_img)
    sharpened = enhancer.enhance(2.5)
    sharpened.save(os.path.join(debug_dir, "pil_sharpened.png"))

    # TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
    enhancer = ImageEnhance.Contrast(sharpened)
    contrasted = enhancer.enhance(1.8)
    contrasted.save(os.path.join(debug_dir, "pil_contrasted.png"))

    # L√†m m·ªãn nh·∫π
    smoothed = contrasted.filter(ImageFilter.SMOOTH)
    smoothed.save(os.path.join(debug_dir, "pil_smoothed.png"))

    return smoothed

def ocr_with_vietocr(image, detector, page_num, debug_dir="debug_images"):
    """
    Th·ª±c hi·ªán OCR v·ªõi VietOCR, ki·ªÉm tra ch·∫•t l∆∞·ª£ng vƒÉn b·∫£n
    """
    os.makedirs(debug_dir, exist_ok=True)

    try:
        text = detector.predict(image)
        if len(text.strip()) < 10:
            logger.warning(f"Trang {page_num}: VƒÉn b·∫£n qu√° ng·∫Øn ({len(text)} k√Ω t·ª±)")
            return text, "vietocr_short"
        if not any(c.isalnum() for c in text):
            logger.warning(f"Trang {page_num}: VƒÉn b·∫£n kh√¥ng ch·ª©a k√Ω t·ª± ch·ªØ/s·ªë")
            return text, "vietocr_invalid"
        return text, "vietocr"
    except Exception as e:
        logger.error(f"Trang {page_num}: VietOCR th·∫•t b·∫°i ({str(e)})")
        if isinstance(image, Image.Image):
            image.save(os.path.join(debug_dir, f"page_{page_num}_failed.png"))
        return "", "vietocr_failed"


class LegalDocumentInfoExtractor:
    """
    Tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c t·ª´ t√†i li·ªáu ph√°p l√Ω
    """

    def __init__(self):
        # Patterns cho c√°c th√¥ng tin ph√°p l√Ω
        self.patterns = {
            'law_number': [
                r'(?:Lu·∫≠t|B·ªô lu·∫≠t|Ngh·ªã ƒë·ªãnh|Th√¥ng t∆∞|Quy·∫øt ƒë·ªãnh)\s+(?:s·ªë\s*)?(\d+/\d+/[A-Z-]+)',
                r'(\d+/\d+/QH\d+)',
                r'(\d+/\d+/Nƒê-CP)',
                r'(\d+/\d+/TT-[A-Z]+)'
            ],
            'dates': [
                r'ng√†y\s+(\d{1,2}/\d{1,2}/\d{4})',
                r'(\d{1,2}\s+th√°ng\s+\d{1,2}\s+nƒÉm\s+\d{4})',
                r'ban h√†nh\s+(?:ng√†y\s+)?(\d{1,2}/\d{1,2}/\d{4})'
            ],
            'chapters': [
                r'CH∆Ø∆†NG\s+([IVX\d]+)[\s\n]+([^\n]+)',
                r'Ch∆∞∆°ng\s+(\d+)[\s\.:]+([^\n]+)'
            ],
            'articles': [
                r'ƒêi·ªÅu\s+(\d+)[\s\.:]+([^\n]+)',
                r'ƒêI·ªÄU\s+(\d+)[\s\n]+([^\n]+)'
            ],
            'sections': [
                r'M·ª•c\s+(\d+)[\s\.:]+([^\n]+)',
                r'Ti·∫øt\s+(\d+)[\s\.:]+([^\n]+)'
            ],
            'subjects': [
                r'(?:ƒëi·ªÅu ch·ªânh|quy ƒë·ªãnh v·ªÅ|li√™n quan ƒë·∫øn)\s+([^\n.;]+)',
                r'Ph·∫°m vi ƒëi·ªÅu ch·ªânh[:\n\s]*([^\n]+)'
            ],
            'effective_date': [
                r'c√≥ hi·ªáu l·ª±c[^\d]*(\d{1,2}/\d{1,2}/\d{4})',
                r'th·ª±c hi·ªán[^\d]*(\d{1,2}/\d{1,2}/\d{4})'
            ]
        }

    def extract_law_info(self, text):
        """
        Tr√≠ch xu·∫•t th√¥ng tin c∆° b·∫£n v·ªÅ lu·∫≠t
        """
        info = {
            'law_numbers': [],
            'dates': [],
            'title': '',
            'chapters': [],
            'articles': [],
            'sections': [],
            'subjects': [],
            'effective_dates': []
        }

        # Tr√≠ch xu·∫•t s·ªë lu·∫≠t
        for pattern in self.patterns['law_number']:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                info['law_numbers'].append(match.group(1))

        # Tr√≠ch xu·∫•t ng√†y th√°ng
        for pattern in self.patterns['dates']:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                info['dates'].append(match.group(1))

        # Tr√≠ch xu·∫•t ch∆∞∆°ng
        for pattern in self.patterns['chapters']:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                info['chapters'].append({
                    'number': match.group(1),
                    'title': match.group(2).strip()
                })

        # Tr√≠ch xu·∫•t ƒëi·ªÅu
        for pattern in self.patterns['articles']:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                info['articles'].append({
                    'number': match.group(1),
                    'title': match.group(2).strip()
                })

        # Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ t√†i li·ªáu (th∆∞·ªùng ·ªü ƒë·∫ßu)
        title_patterns = [
            r'(B·ªò LU·∫¨T[^\n]+)',
            r'(LU·∫¨T[^\n]+)',
            r'(NGH·ªä ƒê·ªäNH[^\n]+)',
            r'(TH√îNG T∆Ø[^\n]+)',
            r'(CH·ªà TH·ªä[^\n]+)',
            r'(HI·∫æN PH√ÅP[^\n]+)',
            r'(QUY·∫æT ƒê·ªäNH[^\n]+)',
            r'(PH√ÅP L·ªÜNH[^\n]+)',
            r'(CH·ªà TH·ªä[^\n]+)',
        ]

        for pattern in title_patterns:
            match = re.search(pattern, text[:500], re.IGNORECASE)
            if match:
                info['title'] = match.group(1).strip()
                break

        # Lo·∫°i b·ªè tr√πng l·∫∑p
        info['law_numbers'] = list(set(info['law_numbers']))
        info['dates'] = list(set(info['dates']))

        return info

    def extract_article_content(self, text):
        """
        Tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt c·ªßa t·ª´ng ƒëi·ªÅu
        """
        articles = []

        # Pattern ƒë·ªÉ t√¨m ƒëi·ªÅu v√† n·ªôi dung
        article_pattern = r'ƒêi·ªÅu\s+(\d+)[\s\.:]+([^\n]+)(?:\n((?:(?!ƒêi·ªÅu\s+\d+)[\s\S])*?))?'

        matches = re.finditer(article_pattern, text, re.IGNORECASE | re.DOTALL)

        for match in matches:
            article_num = match.group(1)
            article_title = match.group(2).strip()
            article_content = match.group(3).strip() if match.group(3) else ""

            # Tr√≠ch xu·∫•t c√°c kho·∫£n trong ƒëi·ªÅu
            clauses = []
            if article_content:
                clause_pattern = r'(\d+)\.\s*([^\n]+(?:\n(?!\d+\.)[^\n]*)*)'
                clause_matches = re.finditer(clause_pattern, article_content)

                for clause_match in clause_matches:
                    clauses.append({
                        'number': clause_match.group(1),
                        'content': clause_match.group(2).strip()
                    })

            articles.append({
                'number': article_num,
                'title': article_title,
                'content': article_content,
                'clauses': clauses
            })

        return articles

    def extract_penalties(self, text):
        """
        Tr√≠ch xu·∫•t th√¥ng tin v·ªÅ x·ª≠ ph·∫°t
        """
        penalties = []

        penalty_patterns = [
            r'ph·∫°t ti·ªÅn t·ª´\s+([\d,.]+)\s*ƒë·∫øn\s+([\d,.]+)\s*(?:ƒë·ªìng|VND)',
            r'ph·∫°t ti·ªÅn\s+([\d,.]+)\s*(?:ƒë·ªìng|VND)',
            r'(?:b·ªã ph·∫°t|ph·∫°t)\s+([^\n.;]+)',
            r'm·ª©c ph·∫°t[:\s]+([\d,.]+)\s*(?:ƒë·ªìng|VND)'
        ]

        for pattern in penalty_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                penalties.append(match.group(0).strip())

        return list(set(penalties))

    def extract_definitions(self, text):
        """
        Tr√≠ch xu·∫•t c√°c ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ
        """
        definitions = {}

        # Pattern cho ƒë·ªãnh nghƒ©a
        definition_patterns = [
            r'([A-Z√Ä-·ª∏][a-z√†-·ªπ\s]+)\s+l√†\s+([^.;]+[.;])',
            r'Trong\s+(?:Lu·∫≠t|Ngh·ªã ƒë·ªãnh|Th√¥ng t∆∞)\s+n√†y[,\s]*([^"]+)"\s+ƒë∆∞·ª£c hi·ªÉu l√†\s+([^.;]+)',
            r'"([^"]+)"\s+(?:ƒë∆∞·ª£c hi·ªÉu l√†|c√≥ nghƒ©a l√†|l√†)\s+([^.;]+)'
        ]

        for pattern in definition_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                term = match.group(1).strip()
                definition = match.group(2).strip()
                definitions[term] = definition

        return definitions

    def search_content(self, text, query, context_chars=200):
        """
        T√¨m ki·∫øm n·ªôi dung v·ªõi ng·ªØ c·∫£nh xung quanh
        """
        results = []
        query_lower = query.lower()
        text_lower = text.lower()

        start = 0
        while True:
            pos = text_lower.find(query_lower, start)
            if pos == -1:
                break

            # L·∫•y ng·ªØ c·∫£nh xung quanh
            context_start = max(0, pos - context_chars)
            context_end = min(len(text), pos + len(query) + context_chars)
            context = text[context_start:context_end]

            # T√¨m ƒëi·ªÅu ch·ª©a k·∫øt qu·∫£ n√†y
            article_match = re.search(r'ƒêi·ªÅu\s+(\d+)', text[:pos][::-1])
            article_num = article_match.group(1)[::-1] if article_match else "Kh√¥ng x√°c ƒë·ªãnh"

            results.append({
                'position': pos,
                'context': context,
                'article': article_num,
                'match_text': text[pos:pos + len(query)]
            })

            start = pos + 1

        return results

    def find_related_articles(self, articles, keyword):
        """
        T√¨m c√°c ƒëi·ªÅu li√™n quan ƒë·∫øn t·ª´ kh√≥a
        """
        related = []
        keyword_lower = keyword.lower()

        for article in articles:
            relevance_score = 0

            # T√¨m trong ti√™u ƒë·ªÅ
            if keyword_lower in article['title'].lower():
                relevance_score += 3

            # T√¨m trong n·ªôi dung
            content_lower = article['content'].lower()
            keyword_count = content_lower.count(keyword_lower)
            relevance_score += keyword_count

            # T√¨m trong c√°c kho·∫£n
            for clause in article['clauses']:
                if keyword_lower in clause['content'].lower():
                    relevance_score += 2

            if relevance_score > 0:
                related.append({
                    'article': article,
                    'relevance_score': relevance_score
                })

        # S·∫Øp x·∫øp theo ƒë·ªô li√™n quan
        related.sort(key=lambda x: x['relevance_score'], reverse=True)
        return related

    def extract_cross_references(self, text):
        """
        Tr√≠ch xu·∫•t c√°c tham chi·∫øu ch√©o gi·ªØa c√°c ƒëi·ªÅu
        """
        references = []

        # Pattern cho tham chi·∫øu
        ref_patterns = [
            r'theo quy ƒë·ªãnh t·∫°i ƒêi·ªÅu\s+(\d+)',
            r'ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒêi·ªÅu\s+(\d+)',
            r'cƒÉn c·ª© ƒêi·ªÅu\s+(\d+)',
            r'ph√π h·ª£p v·ªõi ƒêi·ªÅu\s+(\d+)',
            r'tr·ª´ tr∆∞·ªùng h·ª£p quy ƒë·ªãnh t·∫°i ƒêi·ªÅu\s+(\d+)'
        ]

        for pattern in ref_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # T√¨m ƒëi·ªÅu ch·ª©a tham chi·∫øu n√†y
                context_start = max(0, match.start() - 100)
                context_end = min(len(text), match.end() + 100)
                context = text[context_start:context_end]

                # T√¨m ƒëi·ªÅu ngu·ªìn
                source_match = re.search(r'ƒêi·ªÅu\s+(\d+)', text[:match.start()][::-1])
                source_article = source_match.group(1)[::-1] if source_match else "Kh√¥ng x√°c ƒë·ªãnh"

                references.append({
                    'source_article': source_article,
                    'target_article': match.group(1),
                    'reference_type': pattern.split('\\s+')[0],
                    'context': context.strip()
                })

        return references

    def classify_article_types(self, articles):
        """
        Ph√¢n lo·∫°i c√°c ƒëi·ªÅu theo ch·ª©c nƒÉng
        """
        classification = {
            'definitions': [],  # ƒêi·ªÅu ƒë·ªãnh nghƒ©a
            'principles': [],  # ƒêi·ªÅu nguy√™n t·∫Øc
            'procedures': [],  # ƒêi·ªÅu th·ªß t·ª•c
            'penalties': [],  # ƒêi·ªÅu x·ª≠ ph·∫°t
            'rights': [],  # ƒêi·ªÅu quy·ªÅn l·ª£i
            'obligations': [],  # ƒêi·ªÅu nghƒ©a v·ª•
            'general': []  # ƒêi·ªÅu chung
        }

        # T·ª´ kh√≥a ƒë·ªÉ ph√¢n lo·∫°i
        keywords = {
            'definitions': ['ƒë·ªãnh nghƒ©a', 'hi·ªÉu l√†', 'c√≥ nghƒ©a', 'ƒë∆∞·ª£c g·ªçi l√†'],
            'principles': ['nguy√™n t·∫Øc', 'c∆° b·∫£n', 'cƒÉn b·∫£n', 'chung'],
            'procedures': ['th·ªß t·ª•c', 'tr√¨nh t·ª±', 'quy tr√¨nh', 'h·ªì s∆°'],
            'penalties': ['ph·∫°t', 'x·ª≠ ph·∫°t', 'vi ph·∫°m', 'ch·∫ø t√†i'],
            'rights': ['quy·ªÅn', 'ƒë∆∞·ª£c', 'c√≥ quy·ªÅn'],
            'obligations': ['nghƒ©a v·ª•', 'ph·∫£i', 'kh√¥ng ƒë∆∞·ª£c', 'b·∫Øt bu·ªôc']
        }

        for article in articles:
            article_text = (article['title'] + ' ' + article['content']).lower()
            scores = {}

            for category, category_keywords in keywords.items():
                score = sum(article_text.count(keyword) for keyword in category_keywords)
                scores[category] = score

            # Ph√¢n lo·∫°i theo ƒëi·ªÉm cao nh·∫•t
            best_category = max(scores, key=scores.get) if max(scores.values()) > 0 else 'general'
            classification[best_category].append({
                'article': article,
                'confidence': scores[best_category]
            })

        return classification


def ocr_legal_document_with_extraction(pdf_path, output_dir="ocr_output"):
    """
    OCR t√†i li·ªáu ph√°p l√Ω v·ªõi VietOCR, ch·ªâ d√πng ti·ªÅn x·ª≠ l√Ω m·∫°nh nh·∫•t
    """
    os.makedirs(output_dir, exist_ok=True)

    try:
        config = Cfg.load_config_from_name('vgg_transformer')
        config['device'] = 'cpu'
        config['predictor']['beamsearch'] = True
        detector = Predictor(config)
    except Exception as e:
        logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o VietOCR: {e}")
        return [], {}, ""

    extractor = LegalDocumentInfoExtractor()
    logger.info("üìÑ ƒêang chuy·ªÉn PDF th√†nh ·∫£nh...")
    pdf_images = extract_images_from_pdf_pages(pdf_path, dpi=400)

    ocr_results = []
    all_text = ""

    for img_data in pdf_images:
        page_num = img_data['page_num']
        image = img_data['image']
        debug_dir = os.path.join(output_dir, f"debug_page_{page_num}")

        logger.info(f"üîç ƒêang x·ª≠ l√Ω trang {page_num}...")

        img_array = np.array(image.convert('L'))
        if np.std(img_array) < 10:
            logger.warning(f"Trang {page_num} c√≥ v·∫ª tr·ªëng, b·ªè qua")
            ocr_results.append({
                'page': page_num,
                'best_method': 'none',
                'best_text': '',
                'all_results': {'none': ''}
            })
            continue

        processed = advanced_preprocess_legal_document(image, debug_dir=debug_dir)
        pil_processed = Image.fromarray(processed)
        text, method = ocr_with_vietocr(pil_processed, detector, page_num, debug_dir)

        page_result = {
            'page': page_num,
            'best_method': method,
            'best_text': text,
            'all_results': {method: text}
        }

        ocr_results.append(page_result)
        all_text += text + "\n\n"

        logger.info(f"Trang {page_num} ho√†n th√†nh - Ph∆∞∆°ng ph√°p: {method}, ƒê·ªô d√†i: {len(text)}")

    logger.info("üî¨ ƒêang tr√≠ch xu·∫•t th√¥ng tin ph√°p l√Ω...")
    law_info = extractor.extract_law_info(all_text)
    articles = extractor.extract_article_content(all_text)
    penalties = extractor.extract_penalties(all_text)
    definitions = extractor.extract_definitions(all_text)

    extracted_info = {
        'document_info': law_info,
        'articles': articles,
        'penalties': penalties,
        'definitions': definitions,
        'statistics': {
            'total_pages': len(ocr_results),
            'total_characters': len(all_text),
            'total_articles': len(articles),
            'total_chapters': len(law_info['chapters']),
            'processing_methods_used': Counter([r['best_method'] for r in ocr_results])
        }
    }

    if not all_text.strip():
        logger.warning("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n. Ki·ªÉm tra h√¨nh ·∫£nh debug trong th∆∞ m·ª•c ƒë·∫ßu ra.")

    return ocr_results, extracted_info, all_text


def save_comprehensive_results(ocr_results, extracted_info, all_text, output_dir):
    """
    L∆∞u k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß
    """
    with open(os.path.join(output_dir, "full_text.txt"), 'w', encoding='utf-8') as f:
        f.write(all_text)

    with open(os.path.join(output_dir, "extracted_info.json"), 'w', encoding='utf-8') as f:
        json.dump(extracted_info, f, ensure_ascii=False, indent=2)

    with open(os.path.join(output_dir, "detailed_report.txt"), 'w', encoding='utf-8') as f:
        f.write("=== B√ÅO C√ÅO PH√ÇN T√çCH T√ÄI LI·ªÜU PH√ÅP L√ù ===\n\n")
        f.write("üìä TH·ªêNG K√ä T·ªîNG QUAN:\n")
        stats = extracted_info['statistics']
        f.write(f"- T·ªïng s·ªë trang: {stats['total_pages']}\n")
        f.write(f"- T·ªïng k√Ω t·ª±: {stats['total_characters']:,}\n")
        f.write(f"- S·ªë ƒëi·ªÅu: {stats['total_articles']}\n")
        f.write(f"- S·ªë ch∆∞∆°ng: {stats['total_chapters']}\n\n")
        doc_info = extracted_info['document_info']
        f.write("üìÑ TH√îNG TIN T√ÄI LI·ªÜU:\n")
        if doc_info['title']:
            f.write(f"Ti√™u ƒë·ªÅ: {doc_info['title']}\n")
        if doc_info['law_numbers']:
            f.write(f"S·ªë hi·ªáu: {', '.join(doc_info['law_numbers'])}\n")
        if doc_info['dates']:
            f.write(f"Ng√†y ban h√†nh: {', '.join(doc_info['dates'])}\n\n")
        if doc_info['chapters']:
            f.write("üìö DANH S√ÅCH CH∆Ø∆†NG:\n")
            for chapter in doc_info['chapters']:
                f.write(f"- Ch∆∞∆°ng {chapter['number']}: {chapter['title']}\n")
            f.write("\n")
        if extracted_info['articles']:
            f.write("üìú M·ªòT S·ªê ƒêI·ªÄU QUAN TR·ªåNG:\n")
            for i, article in enumerate(extracted_info['articles'][:10]):
                f.write(f"\nƒêi·ªÅu {article['number']}: {article['title']}\n")
                if article['content']:
                    content_preview = article['content'][:200]
                    f.write(f"N·ªôi dung: {content_preview}...\n")
                if article['clauses']:
                    f.write(f"C√≥ {len(article['clauses'])} kho·∫£n\n")
        if extracted_info['penalties']:
            f.write(f"\nüí∞ TH√îNG TIN X·ª¨ PH·∫†T ({len(extracted_info['penalties'])} m·ª•c):\n")
            for penalty in extracted_info['penalties'][:10]:
                f.write(f"- {penalty}\n")
        if extracted_info['definitions']:
            f.write(f"\nüìñ ƒê·ªäNH NGHƒ®A THU·∫¨T NG·ªÆ ({len(extracted_info['definitions'])} thu·∫≠t ng·ªØ):\n")
            for term, definition in list(extracted_info['definitions'].items())[:10]:
                f.write(f"- {term}: {definition}\n")


def create_interactive_search_system(extracted_info, all_text):
    """
    T·∫°o h·ªá th·ªëng t√¨m ki·∫øm t∆∞∆°ng t√°c
    """
    extractor = LegalDocumentInfoExtractor()

    def search_interface():
        print("\nüîç H·ªÜ TH·ªêNG T√åM KI·∫æM T√ÄI LI·ªÜU PH√ÅP L√ù")
        print("=" * 50)

        while True:
            print("\nC√°c l·ª±a ch·ªçn:")
            print("1. T√¨m ki·∫øm t·ª´ kh√≥a")
            print("2. Tra c·ª©u ƒëi·ªÅu c·ª• th·ªÉ")
            print("3. T√¨m ƒëi·ªÅu li√™n quan")
            print("4. Xem tham chi·∫øu ch√©o")
            print("5. Ph√¢n lo·∫°i ƒëi·ªÅu theo ch·ª©c nƒÉng")
            print("6. Th·ªëng k√™ t√†i li·ªáu")
            print("0. Tho√°t")

            choice = input("\nNh·∫≠p l·ª±a ch·ªçn (0-6): ").strip()

            if choice == "0":
                break
            elif choice == "1":
                keyword = input("Nh·∫≠p t·ª´ kh√≥a c·∫ßn t√¨m: ").strip()
                if keyword:
                    results = extractor.search_content(all_text, keyword)
                    print(f"\nüéØ T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ cho '{keyword}':")
                    for i, result in enumerate(results[:5], 1):
                        print(f"\n{i}. Trong ƒêi·ªÅu {result['article']}:")
                        print(f"   ...{result['context']}...")

            elif choice == "2":
                try:
                    article_num = input("Nh·∫≠p s·ªë ƒëi·ªÅu c·∫ßn tra: ").strip()
                    found_article = next((a for a in extracted_info['articles']
                                          if a['number'] == article_num), None)
                    if found_article:
                        print(f"\nüìú ƒêI·ªÄU {found_article['number']}: {found_article['title']}")
                        print(f"\nN·ªôi dung:\n{found_article['content']}")
                        if found_article['clauses']:
                            print(f"\nC√°c kho·∫£n ({len(found_article['clauses'])}):")
                            for clause in found_article['clauses']:
                                print(f"  {clause['number']}. {clause['content']}")
                    else:
                        print(f"‚ùå Kh√¥ng t√¨m th·∫•y ƒêi·ªÅu {article_num}")
                except:
                    print("‚ùå S·ªë ƒëi·ªÅu kh√¥ng h·ª£p l·ªá")

            elif choice == "3":
                keyword = input("Nh·∫≠p t·ª´ kh√≥a ƒë·ªÉ t√¨m ƒëi·ªÅu li√™n quan: ").strip()
                if keyword:
                    related = extractor.find_related_articles(extracted_info['articles'], keyword)
                    print(f"\nüîó C√°c ƒëi·ªÅu li√™n quan ƒë·∫øn '{keyword}':")
                    for i, item in enumerate(related[:10], 1):
                        article = item['article']
                        score = item['relevance_score']
                        print(f"{i}. ƒêi·ªÅu {article['number']}: {article['title']} (ƒëi·ªÉm: {score})")

            elif choice == "4":
                references = extractor.extract_cross_references(all_text)
                print(f"\nüîÑ Tham chi·∫øu ch√©o ({len(references)} tham chi·∫øu):")
                for ref in references[:20]:
                    print(f"ƒêi·ªÅu {ref['source_article']} ‚Üí ƒêi·ªÅu {ref['target_article']}")
                    print(f"   Ng·ªØ c·∫£nh: {ref['context'][:100]}...")
                    print()

            elif choice == "5":
                classification = extractor.classify_article_types(extracted_info['articles'])
                print("\nüìä PH√ÇN LO·∫†I ƒêI·ªÄU L·∫¨T:")
                for category, articles in classification.items():
                    if articles:
                        print(f"\n{category.upper()} ({len(articles)} ƒëi·ªÅu):")
                        for item in articles[:5]:
                            article = item['article']
                            print(f"  - ƒêi·ªÅu {article['number']}: {article['title']}")

            elif choice == "6":
                stats = extracted_info['statistics']
                doc_info = extracted_info['document_info']
                print("\nüìà TH·ªêNG K√ä T√ÄI LI·ªÜU:")
                print(f"üìÑ T·ªïng trang: {stats['total_pages']}")
                print(f"üìù T·ªïng k√Ω t·ª±: {stats['total_characters']:,}")
                print(f"üìã S·ªë ch∆∞∆°ng: {stats['total_chapters']}")
                print(f"üìú S·ªë ƒëi·ªÅu: {stats['total_articles']}")
                print(f"üí∞ Th√¥ng tin x·ª≠ ph·∫°t: {len(extracted_info['penalties'])}")
                print(f"üìñ ƒê·ªãnh nghƒ©a: {len(extracted_info['definitions'])}")
                if doc_info['law_numbers']:
                    print(f"üè∑Ô∏è  S·ªë hi·ªáu: {', '.join(doc_info['law_numbers'])}")

            else:
                print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")

    return search_interface


def generate_summary_report(extracted_info, all_text):
    """
    T·∫°o b√°o c√°o t√≥m t·∫Øt th√¥ng minh (phi√™n b·∫£n ho√†n ch·ªânh)
    """
    extractor = LegalDocumentInfoExtractor()

    # Ph√¢n t√≠ch n√¢ng cao
    references = extractor.extract_cross_references(all_text)
    classification = extractor.classify_article_types(extracted_info['articles'])
    structure_analysis = analyze_document_structure(extracted_info)

    summary = {
        'executive_summary': '',
        'key_highlights': [],
        'structure_analysis': structure_analysis,
        'important_articles': [],
        'penalty_summary': [],
        'cross_reference_analysis': {},
        'recommendations': []
    }

    # T√≥m t·∫Øt ƒëi·ªÅu h√†nh
    doc_info = extracted_info['document_info']
    stats = extracted_info['statistics']

    summary['executive_summary'] = f"""
T√†i li·ªáu '{doc_info.get('title', 'Kh√¥ng x√°c ƒë·ªãnh')}' ({structure_analysis['document_type']}) 
l√† vƒÉn b·∫£n ph√°p l√Ω g·ªìm {stats['total_articles']} ƒëi·ªÅu ƒë∆∞·ª£c t·ªï ch·ª©c th√†nh {stats['total_chapters']} ch∆∞∆°ng.
ƒê·ªô ph·ª©c t·∫°p: {structure_analysis['complexity_score']:.1f}/10. 
T√†i li·ªáu c√≥ {len(extracted_info['penalties'])} quy ƒë·ªãnh x·ª≠ ph·∫°t v√† 
{len(extracted_info['definitions'])} ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ.
Ch·∫•t l∆∞·ª£ng t·ªï ch·ª©c: {structure_analysis['organization_quality']}.
    """.strip()

    # ƒêi·ªÉm n·ªïi b·∫≠t
    summary['key_highlights'] = [
        f"Lo·∫°i t√†i li·ªáu: {structure_analysis['document_type']}",
        f"ƒê·ªô ph·ª©c t·∫°p: {structure_analysis['complexity_score']:.1f}/10",
        f"C√≥ {len(references)} tham chi·∫øu ch√©o gi·ªØa c√°c ƒëi·ªÅu",
        f"M·∫≠t ƒë·ªô n·ªôi dung: {structure_analysis['content_density']} ƒëi·ªÅu/1000 k√Ω t·ª±",
        f"S·ª©c kh·ªèe c·∫•u tr√∫c: {structure_analysis['structural_health']}"
    ]

    # Ph√¢n t√≠ch c√°c ƒëi·ªÅu quan tr·ªçng
    important_articles = []
    for article in extracted_info['articles'][:10]:  # Top 10 ƒëi·ªÅu ƒë·∫ßu ti√™n
        importance_score = 0

        # ƒêi·ªÅu c√≥ nhi·ªÅu kho·∫£n th∆∞·ªùng quan tr·ªçng
        importance_score += len(article['clauses']) * 0.5

        # ƒêi·ªÅu d√†i th∆∞·ªùng c√≥ n·ªôi dung quan tr·ªçng
        importance_score += len(article['content']) / 1000

        # ƒêi·ªÅu c√≥ t·ª´ kh√≥a quan tr·ªçng
        important_keywords = ['nguy√™n t·∫Øc', 'c∆° b·∫£n', 'quy·ªÅn', 'nghƒ©a v·ª•', 'c·∫•m', 'ph·∫°t']
        content_lower = (article['title'] + ' ' + article['content']).lower()
        for keyword in important_keywords:
            if keyword in content_lower:
                importance_score += 1

        if importance_score > 2:
            important_articles.append({
                'article': article,
                'importance_score': importance_score
            })

    important_articles.sort(key=lambda x: x['importance_score'], reverse=True)
    summary['important_articles'] = important_articles[:5]

    # T√≥m t·∫Øt x·ª≠ ph·∫°t
    penalty_analysis = {
        'total_penalties': len(extracted_info['penalties']),
        'penalty_types': [],
        'fine_ranges': []
    }

    for penalty in extracted_info['penalties']:
        penalty_lower = penalty.lower()
        if 'ph·∫°t ti·ªÅn' in penalty_lower:
            penalty_analysis['penalty_types'].append('Ph·∫°t ti·ªÅn')
        if 't∆∞·ªõc quy·ªÅn' in penalty_lower:
            penalty_analysis['penalty_types'].append('T∆∞·ªõc quy·ªÅn')
        if 'ƒë√¨nh ch·ªâ' in penalty_lower:
            penalty_analysis['penalty_types'].append('ƒê√¨nh ch·ªâ')

    penalty_analysis['penalty_types'] = list(set(penalty_analysis['penalty_types']))
    summary['penalty_summary'] = penalty_analysis

    # Ph√¢n t√≠ch tham chi·∫øu ch√©o
    if references:
        ref_network = defaultdict(list)
        for ref in references:
            ref_network[ref['source_article']].append(ref['target_article'])

        # T√¨m ƒëi·ªÅu ƒë∆∞·ª£c tham chi·∫øu nhi·ªÅu nh·∫•t
        target_counts = Counter()
        for ref in references:
            target_counts[ref['target_article']] += 1

        summary['cross_reference_analysis'] = {
            'total_references': len(references),
            'articles_with_references': len(ref_network),
            'most_referenced_articles': target_counts.most_common(5),
            'reference_density': len(references) / stats['total_articles'] if stats['total_articles'] > 0 else 0
        }

    # Khuy·∫øn ngh·ªã
    recommendations = []

    if structure_analysis['complexity_score'] > 7:
        recommendations.append("T√†i li·ªáu c√≥ ƒë·ªô ph·ª©c t·∫°p cao, n√™n c√≥ h·ªá th·ªëng tra c·ª©u h·ªó tr·ª£")

    if structure_analysis['organization_quality'] == 'C√≥ th·ªÉ qu√° ph·ª©c t·∫°p':
        recommendations.append("C√¢n nh·∫Øc t√°i c·∫•u tr√∫c ƒë·ªÉ gi·∫£m s·ªë ƒëi·ªÅu m·ªói ch∆∞∆°ng")

    if len(extracted_info['definitions']) < 10 and stats['total_articles'] > 50:
        recommendations.append("N√™n b·ªï sung th√™m ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ ƒë·ªÉ tƒÉng t√≠nh r√µ r√†ng")

    if summary['cross_reference_analysis'].get('reference_density', 0) < 0.1:
        recommendations.append("M·∫≠t ƒë·ªô tham chi·∫øu th·∫•p, c√≥ th·ªÉ c·∫ßn li√™n k·∫øt gi·ªØa c√°c ƒëi·ªÅu")

    summary['recommendations'] = recommendations

    return summary


def analyze_document_structure(extracted_info):
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c t√†i li·ªáu
    """
    analysis = {
        'document_type': 'unknown',
        'complexity_score': 0,
        'organization_quality': 'unknown',
        'content_density': 0,
        'structural_health': 'unknown',
        'reference_network': {},
        'content_distribution': {}
    }

    doc_info = extracted_info['document_info']
    stats = extracted_info['statistics']

    # X√°c ƒë·ªãnh lo·∫°i t√†i li·ªáu
    if doc_info['title']:
        title = doc_info['title'].upper()
        if 'B·ªò LU·∫¨T' in title:
            analysis['document_type'] = 'B·ªô lu·∫≠t'
        elif 'LU·∫¨T' in title:
            analysis['document_type'] = 'Lu·∫≠t'
        elif 'NGH·ªä ƒê·ªäNH' in title:
            analysis['document_type'] = 'Ngh·ªã ƒë·ªãnh'
        elif 'TH√îNG T∆Ø' in title:
            analysis['document_type'] = 'Th√¥ng t∆∞'
        elif 'QUY·∫æT ƒê·ªäNH' in title:
            analysis['document_type'] = 'Quy·∫øt ƒë·ªãnh'

    # T√≠nh ƒëi·ªÉm ph·ª©c t·∫°p (thang 10)
    complexity_factors = [
        min(stats['total_articles'] * 0.05, 3),  # S·ªë ƒëi·ªÅu (t·ªëi ƒëa 3 ƒëi·ªÉm)
        min(stats['total_chapters'] * 0.3, 2),  # S·ªë ch∆∞∆°ng (t·ªëi ƒëa 2 ƒëi·ªÉm)
        min(len(extracted_info['penalties']) * 0.1, 2),  # X·ª≠ ph·∫°t (t·ªëi ƒëa 2 ƒëi·ªÉm)
        min(len(extracted_info['definitions']) * 0.05, 1.5),  # ƒê·ªãnh nghƒ©a (t·ªëi ƒëa 1.5 ƒëi·ªÉm)
        min(stats['total_characters'] / 50000, 1.5)  # ƒê·ªô d√†i (t·ªëi ƒëa 1.5 ƒëi·ªÉm)
    ]
    analysis['complexity_score'] = min(sum(complexity_factors), 10)

    # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng t·ªï ch·ª©c
    if stats['total_chapters'] > 0 and stats['total_articles'] > 0:
        avg_articles_per_chapter = stats['total_articles'] / stats['total_chapters']
        if 3 <= avg_articles_per_chapter <= 20:
            analysis['organization_quality'] = 'T·ªët'
        elif avg_articles_per_chapter < 3:
            analysis['organization_quality'] = 'C√≥ th·ªÉ thi·∫øu chi ti·∫øt'
        else:
            analysis['organization_quality'] = 'C√≥ th·ªÉ qu√° ph·ª©c t·∫°p'
    else:
        analysis['organization_quality'] = 'Kh√¥ng c√≥ c·∫•u tr√∫c ch∆∞∆°ng'

    # M·∫≠t ƒë·ªô n·ªôi dung (ƒëi·ªÅu/1000 k√Ω t·ª±)
    if stats['total_characters'] > 0:
        analysis['content_density'] = round(stats['total_articles'] / (stats['total_characters'] / 1000), 2)

    # ƒê√°nh gi√° s·ª©c kh·ªèe c·∫•u tr√∫c
    structure_score = 0
    if stats['total_chapters'] > 0:
        structure_score += 2
    if stats['total_articles'] > 10:
        structure_score += 2
    if len(extracted_info['definitions']) > 5:
        structure_score += 1
    if doc_info['law_numbers']:
        structure_score += 1

    if structure_score >= 5:
        analysis['structural_health'] = 'T·ªët'
    elif structure_score >= 3:
        analysis['structural_health'] = 'Trung b√¨nh'
    else:
        analysis['structural_health'] = 'C·∫ßn c·∫£i thi·ªán'

    # Ph√¢n t√≠ch ph√¢n b·ªë n·ªôi dung
    article_lengths = []
    for article in extracted_info['articles']:
        content_length = len(article['content']) + len(article['title'])
        article_lengths.append(content_length)

    if article_lengths:
        avg_length = sum(article_lengths) / len(article_lengths)
        analysis['content_distribution'] = {
            'average_article_length': round(avg_length),
            'shortest_article': min(article_lengths),
            'longest_article': max(article_lengths),
            'length_variance': 'High' if max(article_lengths) > avg_length * 3 else 'Normal'
        }

    return analysis


def create_detailed_analysis_report(extracted_info, all_text, output_dir):
    """
    T·∫°o b√°o c√°o ph√¢n t√≠ch chi ti·∫øt v·ªõi nhi·ªÅu g√≥c ƒë·ªô
    """
    # Ph√¢n t√≠ch c·∫•u tr√∫c
    structure_analysis = analyze_document_structure(extracted_info)

    # B√°o c√°o t√≥m t·∫Øt
    summary_report = generate_summary_report(extracted_info, all_text)

    # T·∫°o file b√°o c√°o HTML ƒë·∫πp
    html_report = f"""
<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>B√°o C√°o Ph√¢n T√≠ch T√†i Li·ªáu Ph√°p L√Ω</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        .summary-box {{ background: #ecf0f1; padding: 20px; border-radius: 6px; border-left: 4px solid #3498db; margin: 20px 0; }}
        .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }}
        .stat-card {{ background: #fff; padding: 15px; border-radius: 6px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); text-align: center; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #e74c3c; }}
        .stat-label {{ color: #7f8c8d; font-size: 14px; }}
        .article-list {{ background: #f9f9f9; padding: 15px; border-radius: 6px; }}
        .article-item {{ margin: 10px 0; padding: 10px; background: white; border-radius: 4px; }}
        .highlight {{ background: #fff3cd; padding: 2px 4px; border-radius: 3px; }}
        .recommendation {{ background: #d1ecf1; padding: 10px; margin: 5px 0; border-radius: 4px; border-left: 3px solid #bee5eb; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üìä B√°o C√°o Ph√¢n T√≠ch T√†i Li·ªáu Ph√°p L√Ω</h1>

        <div class="summary-box">
            <h2>üìã T√≥m T·∫Øt ƒêi·ªÅu H√†nh</h2>
            <p>{summary_report['executive_summary']}</p>
        </div>

        <h2>üìà Th·ªëng K√™ T·ªïng Quan</h2>
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-number">{extracted_info['statistics']['total_pages']}</div>
                <div class="stat-label">T·ªïng s·ªë trang</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{extracted_info['statistics']['total_articles']}</div>
                <div class="stat-label">S·ªë ƒëi·ªÅu</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{extracted_info['statistics']['total_chapters']}</div>
                <div class="stat-label">S·ªë ch∆∞∆°ng</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(extracted_info['penalties'])}</div>
                <div class="stat-label">Quy ƒë·ªãnh x·ª≠ ph·∫°t</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{structure_analysis['complexity_score']:.1f}/10</div>
                <div class="stat-label">ƒê·ªô ph·ª©c t·∫°p</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{structure_analysis['content_density']}</div>
                <div class="stat-label">M·∫≠t ƒë·ªô n·ªôi dung</div>
            </div>
        </div>

        <h2>üéØ ƒêi·ªÉm N·ªïi B·∫≠t</h2>
        <ul>
            {''.join([f'<li>{highlight}</li>' for highlight in summary_report['key_highlights']])}
        </ul>

        <h2>üìú C√°c ƒêi·ªÅu Quan Tr·ªçng</h2>
        <div class="article-list">
            {''.join([f'''
            <div class="article-item">
                <strong>ƒêi·ªÅu {item['article']['number']}: {item['article']['title']}</strong><br>
                <small>ƒêi·ªÉm quan tr·ªçng: {item['importance_score']:.1f} | S·ªë kho·∫£n: {len(item['article']['clauses'])}</small>
            </div>
            ''' for item in summary_report['important_articles']])}
        </div>

        <h2>üí∞ Ph√¢n T√≠ch X·ª≠ Ph·∫°t</h2>
        <p>T·ªïng s·ªë quy ƒë·ªãnh x·ª≠ ph·∫°t: <span class="highlight">{summary_report['penalty_summary']['total_penalties']}</span></p>
        <p>C√°c lo·∫°i x·ª≠ ph·∫°t: {', '.join(summary_report['penalty_summary']['penalty_types']) if summary_report['penalty_summary']['penalty_types'] else 'Ch∆∞a ph√¢n lo·∫°i'}</p>

        <h2>üîó Ph√¢n T√≠ch Tham Chi·∫øu</h2>
        {f'''
        <p>T·ªïng s·ªë tham chi·∫øu: <span class="highlight">{summary_report['cross_reference_analysis']['total_references']}</span></p>
        <p>M·∫≠t ƒë·ªô tham chi·∫øu: <span class="highlight">{summary_report['cross_reference_analysis']['reference_density']:.2f}</span> tham chi·∫øu/ƒëi·ªÅu</p>
        ''' if 'cross_reference_analysis' in summary_report and summary_report['cross_reference_analysis'] else '<p>Kh√¥ng c√≥ d·ªØ li·ªáu tham chi·∫øu</p>'}

        <h2>üí° Khuy·∫øn Ngh·ªã</h2>
        {''.join([f'<div class="recommendation">‚Ä¢ {rec}</div>' for rec in summary_report['recommendations']]) if summary_report['recommendations'] else '<p>Kh√¥ng c√≥ khuy·∫øn ngh·ªã ƒë·∫∑c bi·ªát</p>'}

        <hr style="margin: 30px 0;">
        <p style="text-align: center; color: #7f8c8d;">
            B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi H·ªá th·ªëng Ph√¢n t√≠ch T√†i li·ªáu Ph√°p l√Ω<br>
            Th·ªùi gian: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
        </p>
    </div>
</body>
</html>
    """

    # L∆∞u b√°o c√°o HTML
    with open(os.path.join(output_dir, "analysis_report.html"), 'w', encoding='utf-8') as f:
        f.write(html_report)

    # L∆∞u b√°o c√°o JSON chi ti·∫øt
    detailed_report = {
        'structure_analysis': structure_analysis,
        'summary_report': summary_report,
        'timestamp': datetime.now().isoformat(),
        'metadata': {
            'total_processing_time': 'N/A',
            'ocr_accuracy_estimate': 'N/A',
            'document_confidence': 'High' if structure_analysis['complexity_score'] < 8 else 'Medium'
        }
    }

    with open(os.path.join(output_dir, "detailed_analysis.json"), 'w', encoding='utf-8') as f:
        json.dump(detailed_report, f, ensure_ascii=False, indent=2)

    return detailed_report


if __name__ == "__main__":
    pdf_path = "../data/CSDLQG/B·ªô lu·∫≠t 45_2019_QH14.pdf"
    output_dir = "legal_analysis_output"

    if os.path.exists(pdf_path):
        logger.info("üöÄ B·∫Øt ƒë·∫ßu OCR v√† ph√¢n t√≠ch t√†i li·ªáu ph√°p l√Ω...")
        ocr_results, extracted_info, all_text = ocr_legal_document_with_extraction(pdf_path, output_dir)
        save_comprehensive_results(ocr_results, extracted_info, all_text, output_dir)

        logger.info(f"\n‚úÖ Ho√†n th√†nh!")
        logger.info(f"üìÑ ƒê√£ x·ª≠ l√Ω {extracted_info['statistics']['total_pages']} trang")
        logger.info(f"üìä Tr√≠ch xu·∫•t ƒë∆∞·ª£c:")
        logger.info(f"   - {extracted_info['statistics']['total_articles']} ƒëi·ªÅu")
        logger.info(f"   - {extracted_info['statistics']['total_chapters']} ch∆∞∆°ng")
        logger.info(f"   - {len(extracted_info['penalties'])} th√¥ng tin x·ª≠ ph·∫°t")
        logger.info(f"   - {len(extracted_info['definitions'])} ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ")
        logger.info(f"üíæ K·∫øt qu·∫£ l∆∞u trong: {output_dir}")

        if extracted_info['document_info']['title']:
            logger.info(f"\nüìã Ti√™u ƒë·ªÅ: {extracted_info['document_info']['title']}")

        if extracted_info['articles']:
            logger.info(
                f"\nüìú ƒêi·ªÅu ƒë·∫ßu ti√™n: ƒêi·ªÅu {extracted_info['articles'][0]['number']} - {extracted_info['articles'][0]['title']}")

        use_search = input("\n‚ùì B·∫°n c√≥ mu·ªën s·ª≠ d·ª•ng h·ªá th·ªëng t√¨m ki·∫øm t∆∞∆°ng t√°c? (y/n): ").strip().lower()
        if use_search in ['y', 'yes', 'c√≥']:
            search_system = create_interactive_search_system(extracted_info, all_text)
            search_system()

        logger.info("\nüéâ C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng ph√¢n t√≠ch t√†i li·ªáu ph√°p l√Ω!")
    else:
        logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {pdf_path}")
        logger.error("Vui l√≤ng ƒë·∫∑t file PDF trong ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c")

